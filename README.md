ğŸ§  Sistema de Reconocimiento de Emociones Faciales con FER2013
Este proyecto implementa un modelo de Deep Learning para clasificar emociones faciales en siete categorÃ­as (enojo, disgusto, miedo, felicidad, tristeza, sorpresa, neutral) utilizando el dataset FER2013. El modelo se construye en Python con TensorFlow/Keras y estÃ¡ diseÃ±ado para ejecutarse en un entorno de Google Colab.
Para lograr un entrenamiento eficiente y un alto rendimiento, se utiliza una tÃ©cnica de Aprendizaje por Transferencia (Transfer Learning) con la arquitectura MobileNetV2, junto con una estrategia de pesos de clase para manejar el desbalance del dataset.

âœ¨ CaracterÃ­sticas
â—	ClasificaciÃ³n de 7 emociones faciales a partir de imÃ¡genes estÃ¡ticas.
â—	Modelo Basado en Transfer Learning: Utiliza MobileNetV2 pre-entrenado para un entrenamiento mÃ¡s rÃ¡pido y preciso.
â—	Optimizado para Google Colab: Facilita el acceso a aceleradores de hardware como GPUs.
â—	Aumento de Datos: Aplica transformaciones a las imÃ¡genes de entrenamiento para mejorar la robustez y reducir el sobreajuste.
â—	Manejo de Desbalance de Clases: Implementa class_weight para asegurar que el modelo preste atenciÃ³n a las emociones con menos muestras.
â—	Interfaz Interactiva: Permite probar el modelo entrenado con imÃ¡genes propias directamente en el notebook.

ğŸ“‹ Requisitos Previos
â—	Una cuenta de Google para usar Google Drive y Google Colab.
â—	El dataset FER2013, que se puede descargar desde Kaggle: Dataset FER2013.

ğŸ› ï¸ ConfiguraciÃ³n del Entorno
Sigue estos pasos para preparar tu entorno de trabajo antes de ejecutar el notebook.
1. Organizar los Archivos en Google Drive
El notebook estÃ¡ configurado para leer el dataset desde una ruta especÃ­fica en tu Google Drive. Debes crear la siguiente estructura de carpetas y colocar tus archivos en ella:
/content/drive/MyDrive/
â””â”€â”€ Colab Notebooks/
    â””â”€â”€ PROYECTO_FER_2013/
        â”œâ”€â”€ emotion_recognition.ipynb  <-- Tu notebook de Colab
        â””â”€â”€ FER2013/
            â”œâ”€â”€ test/
            â”‚   â”œâ”€â”€ angry/
            â”‚   â”œâ”€â”€ disgust/
            â”‚   â”œâ”€â”€ fear/
            â”‚   â”œâ”€â”€ happy/
            â”‚   â”œâ”€â”€ neutral/
            â”‚   â”œâ”€â”€ sad/
            â”‚   â””â”€â”€ surprise/
            â””â”€â”€ train/
                â”œâ”€â”€ angry/
                â”œâ”€â”€ disgust/
                â”œâ”€â”€ fear/
                â”œâ”€â”€ happy/
                â”œâ”€â”€ neutral/
                â”œâ”€â”€ sad/
                â””â”€â”€ surprise/

Nota: Si utilizas una ruta diferente, asegÃºrate de actualizar la variable source_path en la Celda 2 del notebook.
2. Abrir el Notebook en Google Colab
Sube el archivo emotion_recognition.ipynb a la carpeta PROYECTO_FER_2013 en tu Google Drive y Ã¡brelo con Google Colab.

ğŸš€ CÃ³mo Entrenar y Probar el Modelo
El proceso se divide en dos fases principales dentro del notebook.

Fase 1: Entrenamiento del Modelo
1.	Seleccionar un Entorno de EjecuciÃ³n con GPU:
â—‹	En el menÃº de Colab, ve a Entorno de ejecuciÃ³n > Cambiar tipo de entorno de ejecuciÃ³n.
â—‹	En el desplegable "Acelerador de hardware", selecciona T4 GPU. Esto es crucial para un entrenamiento rÃ¡pido.
2.	Ejecutar Todas las Celdas en Orden:
â—‹	La forma mÃ¡s sencilla es ir al menÃº y seleccionar Entorno de ejecuciÃ³n > Ejecutar todas.
â—‹	El notebook realizarÃ¡ los siguientes pasos de forma secuencial:
1.	ImportarÃ¡ todas las librerÃ­as necesarias.
2.	MontarÃ¡ tu Google Drive y copiarÃ¡ el dataset al entorno local de Colab para mayor velocidad.
3.	AnalizarÃ¡ y visualizarÃ¡ la distribuciÃ³n de los datos.
4.	ConfigurarÃ¡ los generadores de datos (ImageDataGenerator), aplicando aumento de datos a las imÃ¡genes de entrenamiento.
5.	ConstruirÃ¡ el modelo MobileNetV2 utilizando Transfer Learning.
6.	CalcularÃ¡ los pesos de clase para manejar el desbalance.
7.	IniciarÃ¡ el proceso de entrenamiento. VerÃ¡s el progreso (precisiÃ³n y pÃ©rdida) en la salida de la celda.
3.	Resultado del Entrenamiento:
â—‹	Durante el entrenamiento, el callback ModelCheckpoint guardarÃ¡ automÃ¡ticamente la mejor versiÃ³n del modelo en un archivo llamado best_emotion_model.h5.
â—‹	Al finalizar, se mostrarÃ¡n grÃ¡ficos con la evoluciÃ³n de la precisiÃ³n y la pÃ©rdida.

Fase 2: Prueba Interactiva con ImÃ¡genes Propias
Una vez que el entrenamiento ha concluido, puedes usar la interfaz interactiva para evaluar el rendimiento del modelo en imÃ¡genes que no pertenecen al dataset.
1.	Ejecuta la Celda de la Interfaz Interactiva:
â—‹	Navega hasta la Ãºltima celda del notebook, titulada "Interfaz Interactiva para Probar con ImÃ¡genes Propias", y ejecÃºtala.
2.	Sube una Imagen:
â—‹	AparecerÃ¡ un botÃ³n para "Elegir archivos". Haz clic y selecciona un archivo de imagen (JPG, PNG, etc.) desde tu computadora.
3.	Analiza los Resultados:
â—‹	El modelo procesarÃ¡ la imagen y mostrarÃ¡:
â– 	La imagen de entrada con la emociÃ³n predicha y el porcentaje de confianza.
â– 	Un grÃ¡fico de barras que detalla la distribuciÃ³n de probabilidad para cada una de las 7 emociones.
4.	Realiza MÃºltiples Pruebas:
â—‹	El script te preguntarÃ¡ si deseas analizar otra imagen. Puedes repetir el proceso tantas veces como quieras para probar diferentes caras y expresiones.

ğŸ—ï¸ TecnologÃ­as Utilizadas
â—	Python 3
â—	Google Colab
â—	TensorFlow / Keras: Framework principal para la construcciÃ³n y entrenamiento del modelo.
â—	Scikit-learn: Utilizado para calcular los pesos de clase (class_weight).
â—	OpenCV: Para el preprocesamiento de las imÃ¡genes subidas en la interfaz interactiva.
â—	NumPy: Para operaciones numÃ©ricas eficientes.
â—	Matplotlib: Para la visualizaciÃ³n de datos y resultados del entrenamiento.
